# Install modules sqlparse and pyyaml for this package to work
# python3 -m pip install pyyaml
# python3 -m pip install sqlparse

import os
import sqlparse
import yaml
from yaml.loader import SafeLoader


def ParseSql(filePath):
    """
    Parse the SQL file and extract detailed data lineage information.
    """
    if not os.path.exists(filePath):
        return None

    with open(filePath, "r") as sql_file:
        sql = sql_file.read()

    # Format the SQL for easier parsing
    formatted_sql = sqlparse.format(
        sql,
        strip_comments=True,
        strip_ws=True,
        indent_columns=True,
        reindent=True,
        keyword_case='upper'
    )

    # Parse and extract detailed lineage
    parsed = sqlparse.parse(formatted_sql)
    table_references = []
    column_references = []
    relationships = []
    ctes = {}

    for statement in parsed:
        tokens = statement.tokens

        for token in tokens:
            token_value = token.value.upper()
            if token.is_group():
                nested_data = parse_sql_group(token)
                table_references.extend(nested_data['tables'])
                column_references.extend(nested_data['columns'])
                relationships.extend(nested_data['relationships'])
                ctes.update(nested_data['ctes'])

            if "FROM" in token_value or "JOIN" in token_value:
                table = get_next_token(tokens, token)
                if table:
                    table_references.append(table.upper())

            if "SELECT" in token_value:
                selected_columns = get_selected_columns(tokens)
                column_references.extend(selected_columns)

            if "ON" in token_value:
                condition = get_condition_after(tokens, token)
                relationships.append(condition)

    return {
        "tables": list(set(table_references)),
        "columns": list(set(column_references)),
        "relationships": relationships,
        "ctes": ctes
    }


def parse_sql_group(token):
    """
    Parse a group of tokens (e.g., subqueries, CTEs).
    """
    table_references = []
    column_references = []
    relationships = []
    ctes = {}

    for subtoken in token.tokens:
        if subtoken.is_group():
            nested_data = parse_sql_group(subtoken)
            table_references.extend(nested_data['tables'])
            column_references.extend(nested_data['columns'])
            relationships.extend(nested_data['relationships'])
            ctes.update(nested_data['ctes'])

        token_value = subtoken.value.upper()

        if "FROM" in token_value or "JOIN" in token_value:
            table = get_next_token(token.tokens, subtoken)
            if table:
                table_references.append(table.upper())

        if "SELECT" in token_value:
            selected_columns = get_selected_columns(token.tokens)
            column_references.extend(selected_columns)

        if "ON" in token_value:
            condition = get_condition_after(token.tokens, subtoken)
            relationships.append(condition)

    return {
        "tables": list(set(table_references)),
        "columns": list(set(column_references)),
        "relationships": relationships,
        "ctes": ctes
    }


def get_next_token(tokens, current_token):
    """
    Get the next token after a specific keyword (e.g., FROM or JOIN).
    """
    try:
        idx = tokens.index(current_token)
        next_token = tokens[idx + 1]
        return next_token.value if next_token else None
    except (ValueError, IndexError):
        return None


def get_selected_columns(tokens):
    """
    Extract columns being selected in a SELECT statement.
    """
    columns = []
    for token in tokens:
        if token.ttype in (sqlparse.tokens.Wildcard, sqlparse.tokens.Name):
            columns.append(token.value)
    return columns


def get_condition_after(tokens, current_token):
    """
    Extract a condition (e.g., ON clause) following a specific token.
    """
    try:
        idx = tokens.index(current_token)
        condition = tokens[idx + 1]
        return condition.value if condition else None
    except (ValueError, IndexError):
        return None


def write_to_csv(lst):
    """
    Write lineage data to a CSV file.
    """
    with open("/Users/vuppupra/Downloads/hl_ongoing.csv", 'w') as csvfile:
        for tbl in lst:
            csvfile.write(str(tbl) + '\n')
    csvfile.close()


def loop_path(path):
    """
    Process all SQL files in the given path and extract lineage details.
    """
    lineage_data = []

    for (root, dirs, files) in os.walk(path):
        for file in files:
            if file.endswith('.sql'):
                filePath = os.path.join(root, file)
                lineage = ParseSql(filePath)
                lineage_data.append({"file": file, "lineage": lineage})

    write_to_csv(lineage_data)
    print(lineage_data)


def getparams(task, dataframe, basePath):
    """
    Extract parameters from the YAML configuration and lineage data.
    """
    nowdf = ("ETL task", "Data Frame", "Table Name", "Filter", "Persist", "URI", "References")
    table = filter = uri = persist = references = ""

    for key in dataframe:
        if key == "table":
            table = dataframe[key]
        elif key == "filter":
            filter = dataframe[key]
        elif key == "uri":
            uri = os.path.join(basePath, dataframe[key])
        elif key == "persist":
            persist = dataframe[key]

        if task == "transform.sql":
            references = ParseSql(uri)

    nowdf = (task, dataframe["name"], table, filter, persist, uri, str(references))
    return nowdf


# Main Execution
YmlFile = "/Users/extract.yaml"
basePath = os.path.dirname(YmlFile)

dataframes = [("ETL task", "Data Frame", "Table Name", "Filter", "Persist", "URI", "References")]
with open(YmlFile, 'r') as stream:
    pipeline = yaml.load(stream, Loader=SafeLoader)

for etl in pipeline:
    for key, value in etl.items():
        if key in ("extract.hive", "load.parquet", "transform.sql"):
            for inp, dfs in value.items():
                for dataframe in dfs:
                    thisdf = getparams(key, dataframe, basePath)
                    print(thisdf)
                    dataframes.append(thisdf)

for aTuple in dataframes:
    elements = '|'.join(map(str, aTuple))
    print(elements)
