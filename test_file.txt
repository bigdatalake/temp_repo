import os
import sqlparse
from sqlparse.sql import Token, IdentifierList, Identifier
from sqlparse.tokens import Keyword, DML
import yaml
from yaml.loader import SafeLoader


# Helper function: Get the next token after a specific keyword
def get_next_token(tokens, current_token):
    try:
        idx = tokens.index(current_token)
        if idx + 1 < len(tokens):
            next_token = tokens[idx + 1]
            return next_token.value if next_token else None
    except ValueError as e:
        print(f"[DEBUG] Error finding next token: {e}")
    return None


# Helper function: Extract selected columns
def get_selected_columns(tokens):
    columns = []
    inside_select = False
    for token in tokens:
        if token.ttype is DML and token.value.upper() == "SELECT":
            inside_select = True
        elif inside_select:
            if isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    columns.append(identifier.get_real_name())
            elif isinstance(token, Identifier):
                columns.append(token.get_real_name())
            if token.ttype is Keyword and token.value.upper() == "FROM":
                break
    return columns


# Helper function: Extract conditions after "ON"
def get_condition_after(tokens, current_token):
    try:
        idx = tokens.index(current_token)
        if idx + 1 < len(tokens):
            condition_token = tokens[idx + 1]
            return condition_token.value.strip() if condition_token else None
    except ValueError as e:
        print(f"[DEBUG] Error extracting condition: {e}")
    return None


# Main function: Parse SQL for tables, columns, relationships, and CTEs
def ParseSql(filePath):
    if not os.path.exists(filePath):
        print(f"[DEBUG] File does not exist: {filePath}")
        return None

    with open(filePath, "r") as sql_file:
        sql = sql_file.read()
        if not sql.strip():
            print(f"[DEBUG] File is empty: {filePath}")
            return None

    try:
        formatted_sql = sqlparse.format(
            sql,
            strip_comments=True,
            strip_ws=True,
            indent_columns=True,
            reindent=True,
            keyword_case='upper'
        )
    except Exception as e:
        print(f"[DEBUG] Error formatting SQL: {e}")
        return None

    try:
        parsed = sqlparse.parse(formatted_sql)
        if not parsed:
            print(f"[DEBUG] SQL parsing returned no statements for file: {filePath}")
            return None
    except Exception as e:
        print(f"[DEBUG] Error parsing SQL: {e}")
        return None

    table_references = []
    column_references = []
    relationships = []
    ctes = {}

    for statement in parsed:
        tokens = statement.tokens
        print(f"[DEBUG] Processing statement: {statement}")

        for token in tokens:
            try:
                token_value = token.value.upper()
                print(f"[DEBUG] Token: {token_value}")

                # Check if the token is a group safely
                if hasattr(token, "is_group") and callable(getattr(token, "is_group")) and token.is_group():
                    continue

                # Check for table references after FROM or JOIN
                if "FROM" in token_value or "JOIN" in token_value:
                    table = get_next_token(tokens, token)
                    if table:
                        table_references.append(table.upper())

                # Check for relationships in ON clauses
                if "ON" in token_value:
                    condition = get_condition_after(tokens, token)
                    if condition:
                        relationships.append(condition)
            except AttributeError as e:
                print(f"[DEBUG] AttributeError while processing token: {e}")
            except Exception as e:
                print(f"[DEBUG] Unexpected error: {e}")

    if not table_references and not column_references and not relationships:
        print(f"[DEBUG] No data lineage information found in file: {filePath}")
        return None

    return {
        "tables": list(set(table_references)),
        "columns": list(set(column_references)),
        "relationships": relationships,
        "ctes": ctes
    }


# Write results to CSV
def write_to_csv(lst, output_path="/Users/vuppupra/Downloads/hl_ongoing.csv"):
    with open(output_path, 'w') as csvfile:
        for item in lst:
            csvfile.write(str(item) + '\n')
    print(f"[DEBUG] CSV written to: {output_path}")


# Loop through a directory and process SQL files
def loop_path(path):
    lineage_data = []

    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith('.sql'):
                file_path = os.path.join(root, file)
                print(f"[DEBUG] Processing file: {file_path}")
                lineage = ParseSql(file_path)
                if lineage:
                    lineage_data.append(lineage)

    write_to_csv(lineage_data)
    print("[DEBUG] Lineage Data:", lineage_data)


# Extract ETL pipeline parameters
def getparams(task, dataframe, basePath):
    table = filter_condition = uri = persist = references = ""
    for key, value in dataframe.items():
        if key == "table":
            table = value
        elif key == "filter":
            filter_condition = value
        elif key == "uri":
            uri = os.path.join(basePath, value)
        elif key == "persist":
            persist = value

        if task == "transform.sql" and uri:
            references = ParseSql(uri)

    return (task, dataframe.get("name"), table, filter_condition, persist, uri, str(references))


# Main function to parse YAML and SQL files
def main():
    YmlFile = "/Users/extract.yaml"
    basePath = os.path.dirname(YmlFile)

    dataframes = [("ETL task", "Data Frame", "Table Name", "Filter", "Persist", "URI", "References")]
    with open(YmlFile, 'r') as stream:
        pipeline = yaml.load(stream, Loader=SafeLoader)

    for etl in pipeline:
        for key, value in etl.items():
            if key in {"extract.hive", "load.parquet", "transform.sql"}:
                for _, dfs in value.items():
                    for dataframe in dfs:
                        thisdf = getparams(key, dataframe, basePath)
                        print(thisdf)
                        dataframes.append(thisdf)

    for aTuple in dataframes:
        print('|'.join(map(str, aTuple)))


# Run the main function
if __name__ == "__main__":
    main()
