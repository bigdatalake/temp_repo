import java.time.{Instant, ZoneId, LocalDateTime, ZoneOffset}
import java.time.format.DateTimeFormatter
import java.util.Properties
import org.apache.kafka.clients.admin.{AdminClient, ListOffsetsResult, OffsetSpec}
import org.apache.kafka.common.TopicPartition
import org.apache.spark.sql.SparkSession


import scala.jdk.CollectionConverters._

object KafkaSparkDataReconApp {

  def main(args: Array[String]): Unit = {
    val topic = "test_data_topic"
    //val startingTime = "2025-07-15 20:16:42.217"
    //val endingTime = "2025-07-15 20:18:46.161"

    val startingTime = "2025-07-15 00:00:00.001"
    val endingTime = "2025-07-15 23:59:59.999"

    // Convert to milliseconds
    val formatter = java.time.format.DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")
    val zoneId = ZoneId.of("Australia/Sydney") // Adjust to your timezone if needed

    val startingEpochMs = LocalDateTime.parse(startingTime, formatter)
      .atZone(zoneId)
      .toInstant
      .toEpochMilli
    val endingEpochMs = LocalDateTime.parse(endingTime, formatter)
      .atZone(zoneId)
      .toInstant
      .toEpochMilli

    // Setup Kafka Admin Client
    val props = new Properties()
    props.put("bootstrap.servers", "localhost:9092")
    val admin = AdminClient.create(props)

    // Get partitions
    val partitions = admin.describeTopics(List(topic).asJava).all().get().get(topic).partitions().asScala.map(_.partition())

    // Create TopicPartitions
    val topicPartitions = partitions.map(p => new TopicPartition(topic, p))

    // Create timestamp mapping
    val timestampsToSearch = topicPartitions.map(tp => tp -> OffsetSpec.forTimestamp(startingEpochMs)).toMap.asJava
    val endTimestampsToSearch = topicPartitions.map(tp => tp -> OffsetSpec.forTimestamp(endingEpochMs)).toMap.asJava

    // Get Offsets
    val startOffsets = admin.listOffsets(timestampsToSearch).all().get()
    val endOffsets = admin.listOffsets(endTimestampsToSearch).all().get()

    // Filter valid TopicPartitions (where start and end offsets are valid)
    val validTopicPartitions = topicPartitions.filter { tp =>
      val startOffset = startOffsets.get(tp).offset()
      val endOffset = endOffsets.get(tp).offset()
      startOffset != -1L && endOffset != -1L
    }

    if (validTopicPartitions.isEmpty) {
      println("No valid partitions found with data in the specified time range.")
      admin.close()
      return
    }

    val fixedStartOffsets = topicPartitions.map { tp =>
      val startOffset = startOffsets.get(tp).offset()
      val correctedStartOffset = if (startOffset == -1L) {
        println(s"Partition ${tp.partition()} has no start offset, fallback to earliest.")
        admin.listOffsets(Map(tp -> OffsetSpec.earliest()).asJava).all().get().get(tp).offset()
      } else startOffset
      tp.partition() -> correctedStartOffset
    }.toMap

    val fixedEndOffsets = topicPartitions.map { tp =>
      val endOffset = endOffsets.get(tp).offset()
      val correctedEndOffset = if (endOffset == -1L) {
        println(s"Partition ${tp.partition()} has no end offset, fallback to latest.")
        admin.listOffsets(Map(tp -> OffsetSpec.latest()).asJava).all().get().get(tp).offset()
      } else endOffset
      tp.partition() -> correctedEndOffset
    }.toMap


    val startingOffsetsJson = fixedStartOffsets.map { case (p, offset) => s""""$p": $offset""" }.mkString("{", ",", "}")
    val endingOffsetsJson = fixedEndOffsets.map { case (p, offset) => s""""$p": $offset""" }.mkString("{", ",", "}")


    val startingOffsetsFullJson = s"""{"$topic": $startingOffsetsJson}"""
    val endingOffsetsFullJson = s"""{"$topic": $endingOffsetsJson}"""

    println(s"startingOffsets = $startingOffsetsFullJson")
    println(s"endingOffsets = $endingOffsetsFullJson")


    admin.close()

    val spark = SparkSession.builder()
      .appName("KafkaDataRecon")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._
    spark.sparkContext.setLogLevel("ERROR")

    val kafkaDf = spark.read
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("startingOffsets", startingOffsetsFullJson)
      .option("endingOffsets", endingOffsetsFullJson)
      .option("subscribe", topic)
      .load()

    // Extract and process data
    val parsedDf = kafkaDf.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", "timestamp", "topic", "partition", "offset")
    //.withColumn("record_hash", sha2(concat_ws("|", $"key", $"value", $"timestamp", $"topic"), 256))
    //.withColumn("record_id", concat($"record_hash", lit("_"), $"timestamp"))

    // Data reconciliation logic (example: count records)
    val count = parsedDf.count()

    println(s"Total records between offsets: $count")

    // Optionally write to disk for reconciliation output
    //parsedDf.write.mode("overwrite").json("/Users/you/output/kafka-batch-data")
    parsedDf.show(false)

  }
}
